{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chequeos previos de performance\n",
    "\n",
    "Para evitar tiempos de entrenamiento muy extensos es recomendable utilizar la **GPU** disponible en el sistema en lugar de la **CPU**, ya que posee la habilidad de realizar cálculos matemáticos de manera más eficiente. A continuación se verifica que se esté efectivamente utilizando la GPU en lugar de la CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5394490448753632075\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3178453401\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18346702169759372435\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: No fue posible descargar el dataset desde el link propiciado en el eunciado de esta pregunta, por lo que se optó por utilizar el dataset (que se asume es análogo) desde keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import keras\n",
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "(x_train, y_train), (x_original_test, y_original_test) = cifar10.load_data()\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_original_test = keras.utils.to_categorical(y_original_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de conjuntos de validación, test y entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan 3 conjuntos distintos para los experimentos a realizar en esta pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000\n",
      "Test samples: 5000\n",
      "Validation samples: 5000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_original_test, y_original_test, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "print(\"Train samples: %d\" % len(x_train))\n",
    "print(\"Test samples: %d\" % len(x_test))\n",
    "print(\"Validation samples: %d\" % len(x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cambios de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se visualiza el esquema de la red neuronal convolucional a utilizar para el siguiente experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,141,514\n",
      "Trainable params: 2,141,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "def define_convolutional(filter_size=3):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (filter_size, filter_size), padding='same', input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (filter_size, filter_size), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "model = define_convolutional()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede notar que a medida que se propaga la información de las imágenes a través de la red, esta sufre una modificación en su representación según una serie de transformaciones denominadas convoluciones, que permite que cada neurona reciba una porción de la imágen original, y que la capa subsiguiente pueda entonces detectar características específicas es estas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de red usando SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el entrenamiento con un decay customizado definido en `step_decay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 12s 232us/step - loss: 14.5062 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 11s 213us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 11s 214us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4998 - val_acc: 0.1004\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, rmsprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    lrate = initial_lrate * math.pow(0.5, math.floor((1+epoch)/5))\n",
    "    lrate = max(lrate,0.00001)\n",
    "    return lrate\n",
    "\n",
    "# Entrenamiento de la red\n",
    "model = define_convolutional()\n",
    "sgd = SGD(lr=0.0, momentum=0.9, decay=0.0)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test,y_test), shuffle=True, callbacks=[lrate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar claramente en el output de keras que el error se mantiene estático durante todas las epochs y con un accuracy de `0.1`, por tanto es irrelevante graficar el cambio de la función de perdida. Esto evidencia un problema con los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de red utilizando RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 13s 254us/step - loss: 14.5061 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 13s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 13s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 13s 251us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 13s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4386 - val_acc: 0.1042\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, rmsprop\n",
    "model = define_convolutional()\n",
    "opt = rmsprop(lr=0.001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "history = model.fit(x_train, y_train,batch_size=32,epochs=25, validation_data=(x_test, y_test),shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Evalúe el efecto de modificar el tamaño de los filtros (de convolución) reportando la sensibilidad del error de pruebas a estos cambios en dos tipos de arquitecturas, una profunda y otra no. Presente un gráfico o tabla resumen. Por simplicidad entre durante sólo 15-20 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efecto del tamaño de los filtros de convolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_shallow_convolutional(filter_size=3):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (filter_size, filter_size), padding='same', input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with filter size 2\n",
      "Training with filter size 3\n",
      "Training with filter size 4\n",
      "Training with filter size 5\n",
      "Training with filter size 6\n",
      "Training with filter size 7\n",
      "Training with filter size 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filter_results = {\n",
    "    \"shallow\": [],\n",
    "    \"deep\": []\n",
    "}\n",
    "\n",
    "filter_sizes = range(2, 9)\n",
    "\n",
    "for filter_size in filter_sizes:\n",
    "    print(\"Training with filter size %d\" % filter_size)\n",
    "    # Use the shallow first\n",
    "    model = define_shallow_convolutional(filter_size=filter_size)\n",
    "    model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    history = model.fit(x_train, y_train,batch_size=32,epochs=20, validation_data=(x_test, y_test),shuffle=True, verbose=False)\n",
    "    filter_results[\"shallow\"].append(history)\n",
    "    \n",
    "    # Deep one\n",
    "    model = define_convolutional(filter_size=filter_size)\n",
    "    model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    history = model.fit(x_train, y_train,batch_size=32,epochs=20, validation_data=(x_test, y_test),shuffle=True, verbose=False)\n",
    "    filter_results[\"deep\"].append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counter = 0\n",
    "plt.figure(figsize=(10, 10))\n",
    "for filter_size in filter_sizes:\n",
    "    res = filter_results[\"shallow\"][counter]\n",
    "    plt.plot(range(0, len(res)), res, label=\"LR %f\" % (lr))\n",
    "    counter += 1\n",
    "    \n",
    "plt.legend()\n",
    "#plt.xlim(-2, 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g)Se ha sugerido que la práctica bastante habitual de continuar una capa convolucional con una capa de pooling puede generar una reducción prematura de las dimensiones del patrón de entrada. Experimente con una arquitectura del tipo  C×C×P×C×C×P×F×F. Use 64 filtros para las primeras 2 capas convolucionales y 128 para las últimas dos. Reflexione sobre qué le parece más sensato: ¿mantener el tamaño de los filtros usados anteriormente? o ¿usar filtros más grandes en la segunda capa convolucional y más pequeños en la primera? o ¿usar filtros más pequeños en la segunda capa convolucional y más grandes en la primera? Hint: con esta nueva arquitectura debiese superar el 70% de accuracy (de validación/test) antes de 5 epochs, pero la arquitectura es más sensible a overfitting por lo que podrı́a ser conveniente agregar un regularizador. Como resultado final de esta actividad gráficque los errores de entrenamiento, validación y pruebas como función del número de “epochs” (fijando el máximo en un número razonable como T = 25).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doble capa de convolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_50 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_114 (Activation)  (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_117 (Activation)  (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "activation_118 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_119 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,460,106\n",
      "Trainable params: 4,460,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_convolutional_for_g():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # C\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # C\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # P\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "     # C\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # C\n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # P\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # F\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # F\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "define_convolutional_for_g().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 14.5059 - acc: 0.0999 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 107s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 108s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 113s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 124s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 121s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 112s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 116s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 121s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 122s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 120s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 121s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 110s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 105s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 105s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 105s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 107s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 107s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 107s 2ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5965 - val_acc: 0.0944\n"
     ]
    }
   ],
   "source": [
    "model = define_convolutional_for_g()\n",
    "opt = rmsprop(lr=0.001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "history = model.fit(x_train, y_train,batch_size=32,epochs=25, validation_data=(x_test, y_test),shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h)Algunos investigadores, han propuesto que las capas de pooling se pueden reemplazar por capas convoluciones con stride 2. ¿Se reduce dimensionalidad de este modo? Compruébelo verificando los cambios de forma (dimensionalidad) que experimenta un patrón de entrada a medida que se ejecuta un forward-pass. Entrene la red resultante con el método que prefiera, gráficando los errores de entrenamiento, validación y pruebas como función del número de “epochs” (fijando el máximo en un número razonable como T = 25)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilización de convolución con strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_58 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_126 (Activation)  (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_127 (Activation)  (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_128 (Activation)  (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_129 (Activation)  (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "activation_130 (Activation)  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_131 (Activation)  (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,644,618\n",
      "Trainable params: 4,644,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_convolutional_for_h():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # C\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # C\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # C_STRIDE\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n",
    "    \n",
    "     # C\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # C\n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # C_STRIDE\n",
    "    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # F\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # F\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "define_convolutional_for_h().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 14.5042 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 208s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 210s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 209s 4ms/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n"
     ]
    }
   ],
   "source": [
    "model = define_convolutional_for_h()\n",
    "opt = rmsprop(lr=0.001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "history = model.fit(x_train, y_train,batch_size=32,epochs=25, validation_data=(x_test, y_test),shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de suconjuntos con movimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La manipulación de las imágenes originales a través de rotaciones y traslaciones en distintos ejes ayuda a que la red neuronal sea mas robusta, y pueda así reconocer los elementos deseados en distintas situaciones. Por ejemplo, si la red neuronal solo aprende a reconocer gatos en una posición específica, es posible que tenga problemas reconociendo gatos en posiciones distintas. Basta que la imágen rote un poco para que la red neuronal tenga problemas al clasificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "  13/1562 [..............................] - ETA: 2:27 - loss: 14.1808 - acc: 0.1202"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False, # set input mean to 0 over the dataset\n",
    "    samplewise_center=False, # set each sample mean to 0\n",
    "    featurewise_std_normalization=False, # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False, # divide each input by its std\n",
    "    zca_whitening=False, # apply ZCA whitening\n",
    "    rotation_range=0, # randomly rotate images (degrees, 0 to 180)\n",
    "    width_shift_range=0.1, # randomly shift images horizontally (fraction of width)\n",
    "    height_shift_range=0.1, # randomly shift images vertically (fraction of height)\n",
    "    horizontal_flip=True, # randomly flip images\n",
    "    vertical_flip=False) # randomly flip images\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "model = define_convolutional()\n",
    "opt = rmsprop(lr=0.001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "batch_size = 32\n",
    "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),steps_per_epoch=x_train.shape[0]// batch_size, epochs=25,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j)Elija una de las redes entrenadas en esta sección (preferentemente una con buen desempeño) y determine los pares de objetos (por ejemplo “camiones” con “autos”) que la red tiende a confundir. Conjeture el motivo de tal confusión. **Hay que hacer una matriz de confusión**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de confusión de red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/25\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 14.5165 - acc: 0.0993 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 2/25\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 3/25\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 4/25\n",
      "50000/50000 [==============================] - 23s 457us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 5/25\n",
      "50000/50000 [==============================] - 23s 456us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 6/25\n",
      "50000/50000 [==============================] - 23s 455us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 7/25\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 8/25\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 9/25\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 10/25\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 11/25\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 12/25\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 13/25\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 14/25\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 15/25\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 16/25\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 17/25\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 18/25\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 19/25\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 20/25\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 21/25\n",
      "50000/50000 [==============================] - 25s 493us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 22/25\n",
      "50000/50000 [==============================] - 25s 491us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 23/25\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 24/25\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n",
      "Epoch 25/25\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 14.5063 - acc: 0.1000 - val_loss: 14.4354 - val_acc: 0.1044\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, rmsprop\n",
    "model = define_convolutional()\n",
    "opt = rmsprop(lr=0.001, decay=1e-6)\n",
    "model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "history = model.fit(x_train, y_train,batch_size=32,epochs=25, validation_data=(x_test, y_test),shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[  0   0   0   0   0 479   0   0   0   0]\n",
      " [  0   0   0   0   0 486   0   0   0   0]\n",
      " [  0   0   0   0   0 525   0   0   0   0]\n",
      " [  0   0   0   0   0 500   0   0   0   0]\n",
      " [  0   0   0   0   0 506   0   0   0   0]\n",
      " [  0   0   0   0   0 478   0   0   0   0]\n",
      " [  0   0   0   0   0 511   0   0   0   0]\n",
      " [  0   0   0   0   0 528   0   0   0   0]\n",
      " [  0   0   0   0   0 489   0   0   0   0]\n",
      " [  0   0   0   0   0 498   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "y_hat = model.predict(x_val)\n",
    "\n",
    "def onehot_to_number(x):\n",
    "    return np.argmax(x)\n",
    "\n",
    "\n",
    "y_hat_number = list(map(lambda elem: onehot_to_number(elem), y_hat))\n",
    "y_val_number = list(map(lambda elem: onehot_to_number(elem), y_val))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "matrix = confusion_matrix(y_val_number, y_hat_number)\n",
    "plot_confusion_matrix(matrix, label_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de gradientes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
